{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3049: DtypeWarning: Columns (13,14,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(590223, 20)\n",
      "(39674, 11)\n",
      "           start  duration  registersPerThread  staticSMem  dynamicSMem  \\\n",
      "237  9426588.045     6.656                 NaN         NaN          NaN   \n",
      "247  9426818.124     1.664                 NaN         NaN          NaN   \n",
      "257  9426876.747     1.664                 NaN         NaN          NaN   \n",
      "267  9426930.699     1.632                 NaN         NaN          NaN   \n",
      "277  9426974.315     1.664                 NaN         NaN          NaN   \n",
      "\n",
      "         size  throughput  context  stream                name  corrid  \n",
      "237  0.035889    5.265555      1.0     7.0  [CUDA memcpy HtoD]     236  \n",
      "247  0.000244    0.143280      1.0     7.0  [CUDA memcpy HtoD]     245  \n",
      "257  0.000244    0.143280      1.0     7.0  [CUDA memcpy HtoD]     254  \n",
      "267  0.000244    0.146090      1.0     7.0  [CUDA memcpy HtoD]     263  \n",
      "277  0.000244    0.143280      1.0     7.0  [CUDA memcpy HtoD]     272  \n"
     ]
    }
   ],
   "source": [
    "file = '/workspace/logs/nvprof_resnext101_32x48d_b2.csv'\n",
    "df = pd.read_csv(file, skiprows=5,\n",
    "                 names=['start','duration','gridX','gridY','gridZ','blockX','blockY','blockZ',\n",
    "                        'registersPerThread','staticSMem','dynamicSMem','size','throughput',\n",
    "                        'srcMemType','dstMemType','device','context','stream','name','corrid'])\n",
    "# staticSMem - KB, dynamicSMem - KB, size - MB, throughput - GB/s\n",
    "print(df.shape)\n",
    "df.drop(['gridX','gridY','gridZ','blockX','blockY','blockZ','srcMemType','dstMemType','device'], axis=1, inplace=True)\n",
    "df.dropna(subset=['registersPerThread','staticSMem','dynamicSMem','size','throughput'], how='all', inplace=True)\n",
    "\n",
    "# demangling the name\n",
    "df['name'] = df['name'].apply(torch._C._demangle)\n",
    "print(df.shape)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start                 float64\n",
      "duration              float64\n",
      "registersPerThread    float64\n",
      "staticSMem            float64\n",
      "dynamicSMem           float64\n",
      "size                  float64\n",
      "throughput            float64\n",
      "context               float64\n",
      "stream                float64\n",
      "name                   object\n",
      "corrid                  int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.dtypes)\n",
    "# print(df.tail(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6692, 92025, 182627, 273166, 363605]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[91413, 182286, 272825, 363264, 453703]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print(df[(df['size'] > 2.0) & (df['size'] < 2.5) & (df['name'] == '[CUDA memcpy HtoD]')])\n",
    "batchStartIndices = df.index[(df['size'] > 2.0) & (df['size'] < 2.5) & (df['name'] == '[CUDA memcpy HtoD]')].tolist()\n",
    "display(batchStartIndices)\n",
    "\n",
    "# print(df[df['name'].str.contains(\"gatherTopK\")])\n",
    "accComputeStartIndices = df.index[df['name'].str.contains(\"gatherTopK\")].tolist()\n",
    "display(accComputeStartIndices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6465, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>duration</th>\n",
       "      <th>registersPerThread</th>\n",
       "      <th>staticSMem</th>\n",
       "      <th>dynamicSMem</th>\n",
       "      <th>size</th>\n",
       "      <th>throughput</th>\n",
       "      <th>context</th>\n",
       "      <th>stream</th>\n",
       "      <th>name</th>\n",
       "      <th>corrid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>92025</td>\n",
       "      <td>1.089080e+07</td>\n",
       "      <td>311.326</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.343750</td>\n",
       "      <td>7.351838</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>[CUDA memcpy HtoD]</td>\n",
       "      <td>84083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92054</td>\n",
       "      <td>1.089151e+07</td>\n",
       "      <td>1.664</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.008955</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>[CUDA memcpy HtoD]</td>\n",
       "      <td>84108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92104</td>\n",
       "      <td>1.089177e+07</td>\n",
       "      <td>2.464</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>cudnn::gemm::computeOffsetsKernel(cudnn::gemm:...</td>\n",
       "      <td>84158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92110</td>\n",
       "      <td>1.089180e+07</td>\n",
       "      <td>118.784</td>\n",
       "      <td>128.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>volta_scudnn_128x64_relu_medium_nn_v1</td>\n",
       "      <td>84161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92141</td>\n",
       "      <td>1.089194e+07</td>\n",
       "      <td>3.008</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>void at::native::elementwise_kernel&lt;128, 4, at...</td>\n",
       "      <td>84189</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              start  duration  registersPerThread  staticSMem  dynamicSMem  \\\n",
       "92025  1.089080e+07   311.326                 NaN         NaN          NaN   \n",
       "92054  1.089151e+07     1.664                 NaN         NaN          NaN   \n",
       "92104  1.089177e+07     2.464                16.0         0.0          0.0   \n",
       "92110  1.089180e+07   118.784               128.0        16.0          0.0   \n",
       "92141  1.089194e+07     3.008                16.0         0.0          0.0   \n",
       "\n",
       "           size  throughput  context  stream  \\\n",
       "92025  2.343750    7.351838      1.0     7.0   \n",
       "92054  0.000015    0.008955      1.0     7.0   \n",
       "92104       NaN         NaN      1.0     7.0   \n",
       "92110       NaN         NaN      1.0     7.0   \n",
       "92141       NaN         NaN      1.0     7.0   \n",
       "\n",
       "                                                    name  corrid  \n",
       "92025                                 [CUDA memcpy HtoD]   84083  \n",
       "92054                                 [CUDA memcpy HtoD]   84108  \n",
       "92104  cudnn::gemm::computeOffsetsKernel(cudnn::gemm:...   84158  \n",
       "92110              volta_scudnn_128x64_relu_medium_nn_v1   84161  \n",
       "92141  void at::native::elementwise_kernel<128, 4, at...   84189  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>duration</th>\n",
       "      <th>registersPerThread</th>\n",
       "      <th>staticSMem</th>\n",
       "      <th>dynamicSMem</th>\n",
       "      <th>size</th>\n",
       "      <th>throughput</th>\n",
       "      <th>context</th>\n",
       "      <th>stream</th>\n",
       "      <th>name</th>\n",
       "      <th>corrid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>182281</td>\n",
       "      <td>1.171870e+07</td>\n",
       "      <td>32.224</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>void at::native::elementwise_kernel&lt;512, 1, at...</td>\n",
       "      <td>166115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182282</td>\n",
       "      <td>1.171874e+07</td>\n",
       "      <td>31.615</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>void at::native::elementwise_kernel&lt;512, 1, at...</td>\n",
       "      <td>166128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182283</td>\n",
       "      <td>1.171878e+07</td>\n",
       "      <td>2.176</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>void at::native::elementwise_kernel&lt;512, 1, at...</td>\n",
       "      <td>166141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182284</td>\n",
       "      <td>1.171879e+07</td>\n",
       "      <td>1.984</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>void at::native::elementwise_kernel&lt;512, 1, at...</td>\n",
       "      <td>166154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182285</td>\n",
       "      <td>1.171880e+07</td>\n",
       "      <td>1.920</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>void at::native::elementwise_kernel&lt;512, 1, at...</td>\n",
       "      <td>166167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               start  duration  registersPerThread  staticSMem  dynamicSMem  \\\n",
       "182281  1.171870e+07    32.224                16.0         0.0          0.0   \n",
       "182282  1.171874e+07    31.615                16.0         0.0          0.0   \n",
       "182283  1.171878e+07     2.176                16.0         0.0          0.0   \n",
       "182284  1.171879e+07     1.984                16.0         0.0          0.0   \n",
       "182285  1.171880e+07     1.920                16.0         0.0          0.0   \n",
       "\n",
       "        size  throughput  context  stream  \\\n",
       "182281   NaN         NaN      1.0     7.0   \n",
       "182282   NaN         NaN      1.0     7.0   \n",
       "182283   NaN         NaN      1.0     7.0   \n",
       "182284   NaN         NaN      1.0     7.0   \n",
       "182285   NaN         NaN      1.0     7.0   \n",
       "\n",
       "                                                     name  corrid  \n",
       "182281  void at::native::elementwise_kernel<512, 1, at...  166115  \n",
       "182282  void at::native::elementwise_kernel<512, 1, at...  166128  \n",
       "182283  void at::native::elementwise_kernel<512, 1, at...  166141  \n",
       "182284  void at::native::elementwise_kernel<512, 1, at...  166154  \n",
       "182285  void at::native::elementwise_kernel<512, 1, at...  166167  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start = batchStartIndices[1]\n",
    "end = accComputeStartIndices[1] - 1\n",
    "b2df = df.loc[start:end,:]\n",
    "\n",
    "b2df = b2df[~((b2df['name'] == '[CUDA memset]') | (b2df['name'] == '[CUDA memcpy DtoD]') | (b2df['name'] =='[CUDA memcpy DtoH]'))]\n",
    "print(b2df.shape)\n",
    "display(b2df.head())\n",
    "display(b2df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>duration</th>\n",
       "      <th>registersPerThread</th>\n",
       "      <th>staticSMem</th>\n",
       "      <th>dynamicSMem</th>\n",
       "      <th>size</th>\n",
       "      <th>throughput</th>\n",
       "      <th>context</th>\n",
       "      <th>stream</th>\n",
       "      <th>name</th>\n",
       "      <th>corrid</th>\n",
       "      <th>joinID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>182236</td>\n",
       "      <td>1.171244e+07</td>\n",
       "      <td>615.196</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>void at::native::elementwise_kernel&lt;512, 1, at::native::gpu_kernel_impl&lt;at::native::add_kernel_cuda(at::TensorIterator&amp;, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() ...</td>\n",
       "      <td>165530</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182237</td>\n",
       "      <td>1.171306e+07</td>\n",
       "      <td>613.117</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>void at::native::elementwise_kernel&lt;512, 1, at::native::gpu_kernel_impl&lt;at::native::add_kernel_cuda(at::TensorIterator&amp;, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() ...</td>\n",
       "      <td>165543</td>\n",
       "      <td>2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182238</td>\n",
       "      <td>1.171368e+07</td>\n",
       "      <td>2.144</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>void at::native::elementwise_kernel&lt;512, 1, at::native::gpu_kernel_impl&lt;at::native::gpu_kernel_with_scalars&lt;at::native::mul_kernel_cuda(at::TensorIterator&amp;)::{lambda()#1}::operator()() const::{lam...</td>\n",
       "      <td>165556</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182239</td>\n",
       "      <td>1.171369e+07</td>\n",
       "      <td>2.144</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>void at::native::elementwise_kernel&lt;512, 1, at::native::gpu_kernel_impl&lt;at::native::add_kernel_cuda(at::TensorIterator&amp;, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() ...</td>\n",
       "      <td>165569</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182240</td>\n",
       "      <td>1.171370e+07</td>\n",
       "      <td>1.888</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>void at::native::elementwise_kernel&lt;512, 1, at::native::gpu_kernel_impl&lt;at::native::add_kernel_cuda(at::TensorIterator&amp;, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() ...</td>\n",
       "      <td>165582</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182241</td>\n",
       "      <td>1.171371e+07</td>\n",
       "      <td>2.080</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>void at::native::elementwise_kernel&lt;512, 1, at::native::gpu_kernel_impl&lt;at::native::gpu_kernel_with_scalars&lt;at::native::mul_kernel_cuda(at::TensorIterator&amp;)::{lambda()#1}::operator()() const::{lam...</td>\n",
       "      <td>165595</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182242</td>\n",
       "      <td>1.171372e+07</td>\n",
       "      <td>2.144</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>void at::native::elementwise_kernel&lt;512, 1, at::native::gpu_kernel_impl&lt;at::native::add_kernel_cuda(at::TensorIterator&amp;, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() ...</td>\n",
       "      <td>165608</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182243</td>\n",
       "      <td>1.171373e+07</td>\n",
       "      <td>2.016</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>void at::native::elementwise_kernel&lt;512, 1, at::native::gpu_kernel_impl&lt;at::native::add_kernel_cuda(at::TensorIterator&amp;, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() ...</td>\n",
       "      <td>165621</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182244</td>\n",
       "      <td>1.171374e+07</td>\n",
       "      <td>255.998</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>void at::native::elementwise_kernel&lt;512, 1, at::native::gpu_kernel_impl&lt;at::native::gpu_kernel_with_scalars&lt;at::native::mul_kernel_cuda(at::TensorIterator&amp;)::{lambda()#1}::operator()() const::{lam...</td>\n",
       "      <td>165634</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182245</td>\n",
       "      <td>1.171400e+07</td>\n",
       "      <td>365.726</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>void at::native::elementwise_kernel&lt;512, 1, at::native::gpu_kernel_impl&lt;at::native::add_kernel_cuda(at::TensorIterator&amp;, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() ...</td>\n",
       "      <td>165647</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182246</td>\n",
       "      <td>1.171438e+07</td>\n",
       "      <td>365.246</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>void at::native::elementwise_kernel&lt;512, 1, at::native::gpu_kernel_impl&lt;at::native::add_kernel_cuda(at::TensorIterator&amp;, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() ...</td>\n",
       "      <td>165660</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182247</td>\n",
       "      <td>1.171475e+07</td>\n",
       "      <td>2.048</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>void at::native::elementwise_kernel&lt;512, 1, at::native::gpu_kernel_impl&lt;at::native::gpu_kernel_with_scalars&lt;at::native::mul_kernel_cuda(at::TensorIterator&amp;)::{lambda()#1}::operator()() const::{lam...</td>\n",
       "      <td>165673</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182248</td>\n",
       "      <td>1.171476e+07</td>\n",
       "      <td>2.016</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>void at::native::elementwise_kernel&lt;512, 1, at::native::gpu_kernel_impl&lt;at::native::add_kernel_cuda(at::TensorIterator&amp;, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() ...</td>\n",
       "      <td>165686</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182249</td>\n",
       "      <td>1.171477e+07</td>\n",
       "      <td>1.920</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>void at::native::elementwise_kernel&lt;512, 1, at::native::gpu_kernel_impl&lt;at::native::add_kernel_cuda(at::TensorIterator&amp;, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() ...</td>\n",
       "      <td>165699</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182250</td>\n",
       "      <td>1.171478e+07</td>\n",
       "      <td>1.920</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>void at::native::elementwise_kernel&lt;512, 1, at::native::gpu_kernel_impl&lt;at::native::gpu_kernel_with_scalars&lt;at::native::mul_kernel_cuda(at::TensorIterator&amp;)::{lambda()#1}::operator()() const::{lam...</td>\n",
       "      <td>165712</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182251</td>\n",
       "      <td>1.171479e+07</td>\n",
       "      <td>1.920</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>void at::native::elementwise_kernel&lt;512, 1, at::native::gpu_kernel_impl&lt;at::native::add_kernel_cuda(at::TensorIterator&amp;, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() ...</td>\n",
       "      <td>165725</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182252</td>\n",
       "      <td>1.171479e+07</td>\n",
       "      <td>1.952</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>void at::native::elementwise_kernel&lt;512, 1, at::native::gpu_kernel_impl&lt;at::native::add_kernel_cuda(at::TensorIterator&amp;, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() ...</td>\n",
       "      <td>165738</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182253</td>\n",
       "      <td>1.171480e+07</td>\n",
       "      <td>255.742</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>void at::native::elementwise_kernel&lt;512, 1, at::native::gpu_kernel_impl&lt;at::native::gpu_kernel_with_scalars&lt;at::native::mul_kernel_cuda(at::TensorIterator&amp;)::{lambda()#1}::operator()() const::{lam...</td>\n",
       "      <td>165751</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182254</td>\n",
       "      <td>1.171507e+07</td>\n",
       "      <td>365.118</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>void at::native::elementwise_kernel&lt;512, 1, at::native::gpu_kernel_impl&lt;at::native::add_kernel_cuda(at::TensorIterator&amp;, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() ...</td>\n",
       "      <td>165764</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182255</td>\n",
       "      <td>1.171544e+07</td>\n",
       "      <td>366.942</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>void at::native::elementwise_kernel&lt;512, 1, at::native::gpu_kernel_impl&lt;at::native::add_kernel_cuda(at::TensorIterator&amp;, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() ...</td>\n",
       "      <td>165777</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182256</td>\n",
       "      <td>1.171581e+07</td>\n",
       "      <td>2.208</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>void at::native::elementwise_kernel&lt;512, 1, at::native::gpu_kernel_impl&lt;at::native::gpu_kernel_with_scalars&lt;at::native::mul_kernel_cuda(at::TensorIterator&amp;)::{lambda()#1}::operator()() const::{lam...</td>\n",
       "      <td>165790</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182257</td>\n",
       "      <td>1.171582e+07</td>\n",
       "      <td>2.240</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>void at::native::elementwise_kernel&lt;512, 1, at::native::gpu_kernel_impl&lt;at::native::add_kernel_cuda(at::TensorIterator&amp;, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() ...</td>\n",
       "      <td>165803</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182258</td>\n",
       "      <td>1.171583e+07</td>\n",
       "      <td>2.048</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>void at::native::elementwise_kernel&lt;512, 1, at::native::gpu_kernel_impl&lt;at::native::add_kernel_cuda(at::TensorIterator&amp;, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() ...</td>\n",
       "      <td>165816</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182259</td>\n",
       "      <td>1.171584e+07</td>\n",
       "      <td>2.112</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>void at::native::elementwise_kernel&lt;512, 1, at::native::gpu_kernel_impl&lt;at::native::gpu_kernel_with_scalars&lt;at::native::mul_kernel_cuda(at::TensorIterator&amp;)::{lambda()#1}::operator()() const::{lam...</td>\n",
       "      <td>165829</td>\n",
       "      <td>2026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182260</td>\n",
       "      <td>1.171585e+07</td>\n",
       "      <td>2.080</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>void at::native::elementwise_kernel&lt;512, 1, at::native::gpu_kernel_impl&lt;at::native::add_kernel_cuda(at::TensorIterator&amp;, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() ...</td>\n",
       "      <td>165842</td>\n",
       "      <td>2027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182261</td>\n",
       "      <td>1.171586e+07</td>\n",
       "      <td>2.080</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>void at::native::elementwise_kernel&lt;512, 1, at::native::gpu_kernel_impl&lt;at::native::add_kernel_cuda(at::TensorIterator&amp;, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() ...</td>\n",
       "      <td>165855</td>\n",
       "      <td>2028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182262</td>\n",
       "      <td>1.171587e+07</td>\n",
       "      <td>430.077</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>void at::native::elementwise_kernel&lt;512, 1, at::native::gpu_kernel_impl&lt;at::native::gpu_kernel_with_scalars&lt;at::native::mul_kernel_cuda(at::TensorIterator&amp;)::{lambda()#1}::operator()() const::{lam...</td>\n",
       "      <td>165868</td>\n",
       "      <td>2029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182263</td>\n",
       "      <td>1.171631e+07</td>\n",
       "      <td>615.005</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>void at::native::elementwise_kernel&lt;512, 1, at::native::gpu_kernel_impl&lt;at::native::add_kernel_cuda(at::TensorIterator&amp;, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() ...</td>\n",
       "      <td>165881</td>\n",
       "      <td>2030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182264</td>\n",
       "      <td>1.171693e+07</td>\n",
       "      <td>615.228</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>void at::native::elementwise_kernel&lt;512, 1, at::native::gpu_kernel_impl&lt;at::native::add_kernel_cuda(at::TensorIterator&amp;, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() ...</td>\n",
       "      <td>165894</td>\n",
       "      <td>2031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182265</td>\n",
       "      <td>1.171755e+07</td>\n",
       "      <td>2.176</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>void at::native::elementwise_kernel&lt;512, 1, at::native::gpu_kernel_impl&lt;at::native::gpu_kernel_with_scalars&lt;at::native::mul_kernel_cuda(at::TensorIterator&amp;)::{lambda()#1}::operator()() const::{lam...</td>\n",
       "      <td>165907</td>\n",
       "      <td>2032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182266</td>\n",
       "      <td>1.171756e+07</td>\n",
       "      <td>2.240</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>void at::native::elementwise_kernel&lt;512, 1, at::native::gpu_kernel_impl&lt;at::native::add_kernel_cuda(at::TensorIterator&amp;, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() ...</td>\n",
       "      <td>165920</td>\n",
       "      <td>2033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182267</td>\n",
       "      <td>1.171757e+07</td>\n",
       "      <td>2.048</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>void at::native::elementwise_kernel&lt;512, 1, at::native::gpu_kernel_impl&lt;at::native::add_kernel_cuda(at::TensorIterator&amp;, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() ...</td>\n",
       "      <td>165933</td>\n",
       "      <td>2034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182268</td>\n",
       "      <td>1.171758e+07</td>\n",
       "      <td>2.112</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>void at::native::elementwise_kernel&lt;512, 1, at::native::gpu_kernel_impl&lt;at::native::gpu_kernel_with_scalars&lt;at::native::mul_kernel_cuda(at::TensorIterator&amp;)::{lambda()#1}::operator()() const::{lam...</td>\n",
       "      <td>165946</td>\n",
       "      <td>2035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182269</td>\n",
       "      <td>1.171759e+07</td>\n",
       "      <td>2.240</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>void at::native::elementwise_kernel&lt;512, 1, at::native::gpu_kernel_impl&lt;at::native::add_kernel_cuda(at::TensorIterator&amp;, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() ...</td>\n",
       "      <td>165959</td>\n",
       "      <td>2036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182270</td>\n",
       "      <td>1.171760e+07</td>\n",
       "      <td>2.016</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>void at::native::elementwise_kernel&lt;512, 1, at::native::gpu_kernel_impl&lt;at::native::add_kernel_cuda(at::TensorIterator&amp;, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() ...</td>\n",
       "      <td>165972</td>\n",
       "      <td>2037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182271</td>\n",
       "      <td>1.171761e+07</td>\n",
       "      <td>255.807</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>void at::native::elementwise_kernel&lt;512, 1, at::native::gpu_kernel_impl&lt;at::native::gpu_kernel_with_scalars&lt;at::native::mul_kernel_cuda(at::TensorIterator&amp;)::{lambda()#1}::operator()() const::{lam...</td>\n",
       "      <td>165985</td>\n",
       "      <td>2038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182272</td>\n",
       "      <td>1.171787e+07</td>\n",
       "      <td>364.894</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>void at::native::elementwise_kernel&lt;512, 1, at::native::gpu_kernel_impl&lt;at::native::add_kernel_cuda(at::TensorIterator&amp;, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() ...</td>\n",
       "      <td>165998</td>\n",
       "      <td>2039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182273</td>\n",
       "      <td>1.171824e+07</td>\n",
       "      <td>366.654</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>void at::native::elementwise_kernel&lt;512, 1, at::native::gpu_kernel_impl&lt;at::native::add_kernel_cuda(at::TensorIterator&amp;, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() ...</td>\n",
       "      <td>166011</td>\n",
       "      <td>2040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182274</td>\n",
       "      <td>1.171862e+07</td>\n",
       "      <td>2.144</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>void at::native::elementwise_kernel&lt;512, 1, at::native::gpu_kernel_impl&lt;at::native::gpu_kernel_with_scalars&lt;at::native::mul_kernel_cuda(at::TensorIterator&amp;)::{lambda()#1}::operator()() const::{lam...</td>\n",
       "      <td>166024</td>\n",
       "      <td>2041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182275</td>\n",
       "      <td>1.171863e+07</td>\n",
       "      <td>2.016</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>void at::native::elementwise_kernel&lt;512, 1, at::native::gpu_kernel_impl&lt;at::native::add_kernel_cuda(at::TensorIterator&amp;, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() ...</td>\n",
       "      <td>166037</td>\n",
       "      <td>2042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182276</td>\n",
       "      <td>1.171864e+07</td>\n",
       "      <td>1.952</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>void at::native::elementwise_kernel&lt;512, 1, at::native::gpu_kernel_impl&lt;at::native::add_kernel_cuda(at::TensorIterator&amp;, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() ...</td>\n",
       "      <td>166050</td>\n",
       "      <td>2043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182277</td>\n",
       "      <td>1.171864e+07</td>\n",
       "      <td>1.920</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>void at::native::elementwise_kernel&lt;512, 1, at::native::gpu_kernel_impl&lt;at::native::gpu_kernel_with_scalars&lt;at::native::mul_kernel_cuda(at::TensorIterator&amp;)::{lambda()#1}::operator()() const::{lam...</td>\n",
       "      <td>166063</td>\n",
       "      <td>2044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182278</td>\n",
       "      <td>1.171865e+07</td>\n",
       "      <td>1.952</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>void at::native::elementwise_kernel&lt;512, 1, at::native::gpu_kernel_impl&lt;at::native::add_kernel_cuda(at::TensorIterator&amp;, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() ...</td>\n",
       "      <td>166076</td>\n",
       "      <td>2045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182279</td>\n",
       "      <td>1.171866e+07</td>\n",
       "      <td>1.920</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>void at::native::elementwise_kernel&lt;512, 1, at::native::gpu_kernel_impl&lt;at::native::add_kernel_cuda(at::TensorIterator&amp;, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() ...</td>\n",
       "      <td>166089</td>\n",
       "      <td>2046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182280</td>\n",
       "      <td>1.171867e+07</td>\n",
       "      <td>22.464</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>void at::native::elementwise_kernel&lt;512, 1, at::native::gpu_kernel_impl&lt;at::native::gpu_kernel_with_scalars&lt;at::native::mul_kernel_cuda(at::TensorIterator&amp;)::{lambda()#1}::operator()() const::{lam...</td>\n",
       "      <td>166102</td>\n",
       "      <td>2047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182281</td>\n",
       "      <td>1.171870e+07</td>\n",
       "      <td>32.224</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>void at::native::elementwise_kernel&lt;512, 1, at::native::gpu_kernel_impl&lt;at::native::add_kernel_cuda(at::TensorIterator&amp;, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() ...</td>\n",
       "      <td>166115</td>\n",
       "      <td>2048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182282</td>\n",
       "      <td>1.171874e+07</td>\n",
       "      <td>31.615</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>void at::native::elementwise_kernel&lt;512, 1, at::native::gpu_kernel_impl&lt;at::native::add_kernel_cuda(at::TensorIterator&amp;, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() ...</td>\n",
       "      <td>166128</td>\n",
       "      <td>2049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182283</td>\n",
       "      <td>1.171878e+07</td>\n",
       "      <td>2.176</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>void at::native::elementwise_kernel&lt;512, 1, at::native::gpu_kernel_impl&lt;at::native::gpu_kernel_with_scalars&lt;at::native::mul_kernel_cuda(at::TensorIterator&amp;)::{lambda()#1}::operator()() const::{lam...</td>\n",
       "      <td>166141</td>\n",
       "      <td>2050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182284</td>\n",
       "      <td>1.171879e+07</td>\n",
       "      <td>1.984</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>void at::native::elementwise_kernel&lt;512, 1, at::native::gpu_kernel_impl&lt;at::native::add_kernel_cuda(at::TensorIterator&amp;, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() ...</td>\n",
       "      <td>166154</td>\n",
       "      <td>2051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182285</td>\n",
       "      <td>1.171880e+07</td>\n",
       "      <td>1.920</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>void at::native::elementwise_kernel&lt;512, 1, at::native::gpu_kernel_impl&lt;at::native::add_kernel_cuda(at::TensorIterator&amp;, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() ...</td>\n",
       "      <td>166167</td>\n",
       "      <td>2052</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               start  duration  registersPerThread  staticSMem  dynamicSMem  \\\n",
       "182236  1.171244e+07   615.196                16.0         0.0          0.0   \n",
       "182237  1.171306e+07   613.117                16.0         0.0          0.0   \n",
       "182238  1.171368e+07     2.144                16.0         0.0          0.0   \n",
       "182239  1.171369e+07     2.144                16.0         0.0          0.0   \n",
       "182240  1.171370e+07     1.888                16.0         0.0          0.0   \n",
       "182241  1.171371e+07     2.080                16.0         0.0          0.0   \n",
       "182242  1.171372e+07     2.144                16.0         0.0          0.0   \n",
       "182243  1.171373e+07     2.016                16.0         0.0          0.0   \n",
       "182244  1.171374e+07   255.998                16.0         0.0          0.0   \n",
       "182245  1.171400e+07   365.726                16.0         0.0          0.0   \n",
       "182246  1.171438e+07   365.246                16.0         0.0          0.0   \n",
       "182247  1.171475e+07     2.048                16.0         0.0          0.0   \n",
       "182248  1.171476e+07     2.016                16.0         0.0          0.0   \n",
       "182249  1.171477e+07     1.920                16.0         0.0          0.0   \n",
       "182250  1.171478e+07     1.920                16.0         0.0          0.0   \n",
       "182251  1.171479e+07     1.920                16.0         0.0          0.0   \n",
       "182252  1.171479e+07     1.952                16.0         0.0          0.0   \n",
       "182253  1.171480e+07   255.742                16.0         0.0          0.0   \n",
       "182254  1.171507e+07   365.118                16.0         0.0          0.0   \n",
       "182255  1.171544e+07   366.942                16.0         0.0          0.0   \n",
       "182256  1.171581e+07     2.208                16.0         0.0          0.0   \n",
       "182257  1.171582e+07     2.240                16.0         0.0          0.0   \n",
       "182258  1.171583e+07     2.048                16.0         0.0          0.0   \n",
       "182259  1.171584e+07     2.112                16.0         0.0          0.0   \n",
       "182260  1.171585e+07     2.080                16.0         0.0          0.0   \n",
       "182261  1.171586e+07     2.080                16.0         0.0          0.0   \n",
       "182262  1.171587e+07   430.077                16.0         0.0          0.0   \n",
       "182263  1.171631e+07   615.005                16.0         0.0          0.0   \n",
       "182264  1.171693e+07   615.228                16.0         0.0          0.0   \n",
       "182265  1.171755e+07     2.176                16.0         0.0          0.0   \n",
       "182266  1.171756e+07     2.240                16.0         0.0          0.0   \n",
       "182267  1.171757e+07     2.048                16.0         0.0          0.0   \n",
       "182268  1.171758e+07     2.112                16.0         0.0          0.0   \n",
       "182269  1.171759e+07     2.240                16.0         0.0          0.0   \n",
       "182270  1.171760e+07     2.016                16.0         0.0          0.0   \n",
       "182271  1.171761e+07   255.807                16.0         0.0          0.0   \n",
       "182272  1.171787e+07   364.894                16.0         0.0          0.0   \n",
       "182273  1.171824e+07   366.654                16.0         0.0          0.0   \n",
       "182274  1.171862e+07     2.144                16.0         0.0          0.0   \n",
       "182275  1.171863e+07     2.016                16.0         0.0          0.0   \n",
       "182276  1.171864e+07     1.952                16.0         0.0          0.0   \n",
       "182277  1.171864e+07     1.920                16.0         0.0          0.0   \n",
       "182278  1.171865e+07     1.952                16.0         0.0          0.0   \n",
       "182279  1.171866e+07     1.920                16.0         0.0          0.0   \n",
       "182280  1.171867e+07    22.464                16.0         0.0          0.0   \n",
       "182281  1.171870e+07    32.224                16.0         0.0          0.0   \n",
       "182282  1.171874e+07    31.615                16.0         0.0          0.0   \n",
       "182283  1.171878e+07     2.176                16.0         0.0          0.0   \n",
       "182284  1.171879e+07     1.984                16.0         0.0          0.0   \n",
       "182285  1.171880e+07     1.920                16.0         0.0          0.0   \n",
       "\n",
       "        size  throughput  context  stream  \\\n",
       "182236   NaN         NaN      1.0     7.0   \n",
       "182237   NaN         NaN      1.0     7.0   \n",
       "182238   NaN         NaN      1.0     7.0   \n",
       "182239   NaN         NaN      1.0     7.0   \n",
       "182240   NaN         NaN      1.0     7.0   \n",
       "182241   NaN         NaN      1.0     7.0   \n",
       "182242   NaN         NaN      1.0     7.0   \n",
       "182243   NaN         NaN      1.0     7.0   \n",
       "182244   NaN         NaN      1.0     7.0   \n",
       "182245   NaN         NaN      1.0     7.0   \n",
       "182246   NaN         NaN      1.0     7.0   \n",
       "182247   NaN         NaN      1.0     7.0   \n",
       "182248   NaN         NaN      1.0     7.0   \n",
       "182249   NaN         NaN      1.0     7.0   \n",
       "182250   NaN         NaN      1.0     7.0   \n",
       "182251   NaN         NaN      1.0     7.0   \n",
       "182252   NaN         NaN      1.0     7.0   \n",
       "182253   NaN         NaN      1.0     7.0   \n",
       "182254   NaN         NaN      1.0     7.0   \n",
       "182255   NaN         NaN      1.0     7.0   \n",
       "182256   NaN         NaN      1.0     7.0   \n",
       "182257   NaN         NaN      1.0     7.0   \n",
       "182258   NaN         NaN      1.0     7.0   \n",
       "182259   NaN         NaN      1.0     7.0   \n",
       "182260   NaN         NaN      1.0     7.0   \n",
       "182261   NaN         NaN      1.0     7.0   \n",
       "182262   NaN         NaN      1.0     7.0   \n",
       "182263   NaN         NaN      1.0     7.0   \n",
       "182264   NaN         NaN      1.0     7.0   \n",
       "182265   NaN         NaN      1.0     7.0   \n",
       "182266   NaN         NaN      1.0     7.0   \n",
       "182267   NaN         NaN      1.0     7.0   \n",
       "182268   NaN         NaN      1.0     7.0   \n",
       "182269   NaN         NaN      1.0     7.0   \n",
       "182270   NaN         NaN      1.0     7.0   \n",
       "182271   NaN         NaN      1.0     7.0   \n",
       "182272   NaN         NaN      1.0     7.0   \n",
       "182273   NaN         NaN      1.0     7.0   \n",
       "182274   NaN         NaN      1.0     7.0   \n",
       "182275   NaN         NaN      1.0     7.0   \n",
       "182276   NaN         NaN      1.0     7.0   \n",
       "182277   NaN         NaN      1.0     7.0   \n",
       "182278   NaN         NaN      1.0     7.0   \n",
       "182279   NaN         NaN      1.0     7.0   \n",
       "182280   NaN         NaN      1.0     7.0   \n",
       "182281   NaN         NaN      1.0     7.0   \n",
       "182282   NaN         NaN      1.0     7.0   \n",
       "182283   NaN         NaN      1.0     7.0   \n",
       "182284   NaN         NaN      1.0     7.0   \n",
       "182285   NaN         NaN      1.0     7.0   \n",
       "\n",
       "                                                                                                                                                                                                           name  \\\n",
       "182236  void at::native::elementwise_kernel<512, 1, at::native::gpu_kernel_impl<at::native::add_kernel_cuda(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() ...   \n",
       "182237  void at::native::elementwise_kernel<512, 1, at::native::gpu_kernel_impl<at::native::add_kernel_cuda(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() ...   \n",
       "182238  void at::native::elementwise_kernel<512, 1, at::native::gpu_kernel_impl<at::native::gpu_kernel_with_scalars<at::native::mul_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lam...   \n",
       "182239  void at::native::elementwise_kernel<512, 1, at::native::gpu_kernel_impl<at::native::add_kernel_cuda(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() ...   \n",
       "182240  void at::native::elementwise_kernel<512, 1, at::native::gpu_kernel_impl<at::native::add_kernel_cuda(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() ...   \n",
       "182241  void at::native::elementwise_kernel<512, 1, at::native::gpu_kernel_impl<at::native::gpu_kernel_with_scalars<at::native::mul_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lam...   \n",
       "182242  void at::native::elementwise_kernel<512, 1, at::native::gpu_kernel_impl<at::native::add_kernel_cuda(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() ...   \n",
       "182243  void at::native::elementwise_kernel<512, 1, at::native::gpu_kernel_impl<at::native::add_kernel_cuda(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() ...   \n",
       "182244  void at::native::elementwise_kernel<512, 1, at::native::gpu_kernel_impl<at::native::gpu_kernel_with_scalars<at::native::mul_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lam...   \n",
       "182245  void at::native::elementwise_kernel<512, 1, at::native::gpu_kernel_impl<at::native::add_kernel_cuda(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() ...   \n",
       "182246  void at::native::elementwise_kernel<512, 1, at::native::gpu_kernel_impl<at::native::add_kernel_cuda(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() ...   \n",
       "182247  void at::native::elementwise_kernel<512, 1, at::native::gpu_kernel_impl<at::native::gpu_kernel_with_scalars<at::native::mul_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lam...   \n",
       "182248  void at::native::elementwise_kernel<512, 1, at::native::gpu_kernel_impl<at::native::add_kernel_cuda(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() ...   \n",
       "182249  void at::native::elementwise_kernel<512, 1, at::native::gpu_kernel_impl<at::native::add_kernel_cuda(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() ...   \n",
       "182250  void at::native::elementwise_kernel<512, 1, at::native::gpu_kernel_impl<at::native::gpu_kernel_with_scalars<at::native::mul_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lam...   \n",
       "182251  void at::native::elementwise_kernel<512, 1, at::native::gpu_kernel_impl<at::native::add_kernel_cuda(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() ...   \n",
       "182252  void at::native::elementwise_kernel<512, 1, at::native::gpu_kernel_impl<at::native::add_kernel_cuda(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() ...   \n",
       "182253  void at::native::elementwise_kernel<512, 1, at::native::gpu_kernel_impl<at::native::gpu_kernel_with_scalars<at::native::mul_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lam...   \n",
       "182254  void at::native::elementwise_kernel<512, 1, at::native::gpu_kernel_impl<at::native::add_kernel_cuda(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() ...   \n",
       "182255  void at::native::elementwise_kernel<512, 1, at::native::gpu_kernel_impl<at::native::add_kernel_cuda(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() ...   \n",
       "182256  void at::native::elementwise_kernel<512, 1, at::native::gpu_kernel_impl<at::native::gpu_kernel_with_scalars<at::native::mul_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lam...   \n",
       "182257  void at::native::elementwise_kernel<512, 1, at::native::gpu_kernel_impl<at::native::add_kernel_cuda(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() ...   \n",
       "182258  void at::native::elementwise_kernel<512, 1, at::native::gpu_kernel_impl<at::native::add_kernel_cuda(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() ...   \n",
       "182259  void at::native::elementwise_kernel<512, 1, at::native::gpu_kernel_impl<at::native::gpu_kernel_with_scalars<at::native::mul_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lam...   \n",
       "182260  void at::native::elementwise_kernel<512, 1, at::native::gpu_kernel_impl<at::native::add_kernel_cuda(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() ...   \n",
       "182261  void at::native::elementwise_kernel<512, 1, at::native::gpu_kernel_impl<at::native::add_kernel_cuda(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() ...   \n",
       "182262  void at::native::elementwise_kernel<512, 1, at::native::gpu_kernel_impl<at::native::gpu_kernel_with_scalars<at::native::mul_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lam...   \n",
       "182263  void at::native::elementwise_kernel<512, 1, at::native::gpu_kernel_impl<at::native::add_kernel_cuda(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() ...   \n",
       "182264  void at::native::elementwise_kernel<512, 1, at::native::gpu_kernel_impl<at::native::add_kernel_cuda(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() ...   \n",
       "182265  void at::native::elementwise_kernel<512, 1, at::native::gpu_kernel_impl<at::native::gpu_kernel_with_scalars<at::native::mul_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lam...   \n",
       "182266  void at::native::elementwise_kernel<512, 1, at::native::gpu_kernel_impl<at::native::add_kernel_cuda(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() ...   \n",
       "182267  void at::native::elementwise_kernel<512, 1, at::native::gpu_kernel_impl<at::native::add_kernel_cuda(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() ...   \n",
       "182268  void at::native::elementwise_kernel<512, 1, at::native::gpu_kernel_impl<at::native::gpu_kernel_with_scalars<at::native::mul_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lam...   \n",
       "182269  void at::native::elementwise_kernel<512, 1, at::native::gpu_kernel_impl<at::native::add_kernel_cuda(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() ...   \n",
       "182270  void at::native::elementwise_kernel<512, 1, at::native::gpu_kernel_impl<at::native::add_kernel_cuda(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() ...   \n",
       "182271  void at::native::elementwise_kernel<512, 1, at::native::gpu_kernel_impl<at::native::gpu_kernel_with_scalars<at::native::mul_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lam...   \n",
       "182272  void at::native::elementwise_kernel<512, 1, at::native::gpu_kernel_impl<at::native::add_kernel_cuda(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() ...   \n",
       "182273  void at::native::elementwise_kernel<512, 1, at::native::gpu_kernel_impl<at::native::add_kernel_cuda(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() ...   \n",
       "182274  void at::native::elementwise_kernel<512, 1, at::native::gpu_kernel_impl<at::native::gpu_kernel_with_scalars<at::native::mul_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lam...   \n",
       "182275  void at::native::elementwise_kernel<512, 1, at::native::gpu_kernel_impl<at::native::add_kernel_cuda(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() ...   \n",
       "182276  void at::native::elementwise_kernel<512, 1, at::native::gpu_kernel_impl<at::native::add_kernel_cuda(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() ...   \n",
       "182277  void at::native::elementwise_kernel<512, 1, at::native::gpu_kernel_impl<at::native::gpu_kernel_with_scalars<at::native::mul_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lam...   \n",
       "182278  void at::native::elementwise_kernel<512, 1, at::native::gpu_kernel_impl<at::native::add_kernel_cuda(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() ...   \n",
       "182279  void at::native::elementwise_kernel<512, 1, at::native::gpu_kernel_impl<at::native::add_kernel_cuda(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() ...   \n",
       "182280  void at::native::elementwise_kernel<512, 1, at::native::gpu_kernel_impl<at::native::gpu_kernel_with_scalars<at::native::mul_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lam...   \n",
       "182281  void at::native::elementwise_kernel<512, 1, at::native::gpu_kernel_impl<at::native::add_kernel_cuda(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() ...   \n",
       "182282  void at::native::elementwise_kernel<512, 1, at::native::gpu_kernel_impl<at::native::add_kernel_cuda(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() ...   \n",
       "182283  void at::native::elementwise_kernel<512, 1, at::native::gpu_kernel_impl<at::native::gpu_kernel_with_scalars<at::native::mul_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lam...   \n",
       "182284  void at::native::elementwise_kernel<512, 1, at::native::gpu_kernel_impl<at::native::add_kernel_cuda(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() ...   \n",
       "182285  void at::native::elementwise_kernel<512, 1, at::native::gpu_kernel_impl<at::native::add_kernel_cuda(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() ...   \n",
       "\n",
       "        corrid  joinID  \n",
       "182236  165530    2003  \n",
       "182237  165543    2004  \n",
       "182238  165556    2005  \n",
       "182239  165569    2006  \n",
       "182240  165582    2007  \n",
       "182241  165595    2008  \n",
       "182242  165608    2009  \n",
       "182243  165621    2010  \n",
       "182244  165634    2011  \n",
       "182245  165647    2012  \n",
       "182246  165660    2013  \n",
       "182247  165673    2014  \n",
       "182248  165686    2015  \n",
       "182249  165699    2016  \n",
       "182250  165712    2017  \n",
       "182251  165725    2018  \n",
       "182252  165738    2019  \n",
       "182253  165751    2020  \n",
       "182254  165764    2021  \n",
       "182255  165777    2022  \n",
       "182256  165790    2023  \n",
       "182257  165803    2024  \n",
       "182258  165816    2025  \n",
       "182259  165829    2026  \n",
       "182260  165842    2027  \n",
       "182261  165855    2028  \n",
       "182262  165868    2029  \n",
       "182263  165881    2030  \n",
       "182264  165894    2031  \n",
       "182265  165907    2032  \n",
       "182266  165920    2033  \n",
       "182267  165933    2034  \n",
       "182268  165946    2035  \n",
       "182269  165959    2036  \n",
       "182270  165972    2037  \n",
       "182271  165985    2038  \n",
       "182272  165998    2039  \n",
       "182273  166011    2040  \n",
       "182274  166024    2041  \n",
       "182275  166037    2042  \n",
       "182276  166050    2043  \n",
       "182277  166063    2044  \n",
       "182278  166076    2045  \n",
       "182279  166089    2046  \n",
       "182280  166102    2047  \n",
       "182281  166115    2048  \n",
       "182282  166128    2049  \n",
       "182283  166141    2050  \n",
       "182284  166154    2051  \n",
       "182285  166167    2052  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# input df should be a dataframe containing entries corresponding to a batch\n",
    "def isNativeRow(name):\n",
    "    \"\"\"\n",
    "    Returns True if a pytorch native operation is performed based on string matching with name attribute.\n",
    "    \"\"\"\n",
    "    return \"at::native::\" in name\n",
    "\n",
    "def isfixIndexJoinID(index):\n",
    "    \"\"\"Return True if index in list. We want to increment joinID by 1 after these to sync with pytorch trace\"\"\"\n",
    "    indices = [93566,97472,102542,122429]\n",
    "    return index in indices\n",
    "\n",
    "def addJoinIDs(df):\n",
    "    df['joinID'] = np.nan\n",
    "    joinID_loc = df.columns.get_loc('joinID')\n",
    "    joinid = 0\n",
    "    \n",
    "    assert df.iloc[0, df.columns.get_loc('name')] == '[CUDA memcpy HtoD]', \"input df doesn't match expected format\"\n",
    "    df.iloc[0, joinID_loc] = joinid\n",
    "    joinid += 1\n",
    "    \n",
    "    assert df.iloc[1, df.columns.get_loc('name')] == '[CUDA memcpy HtoD]', \"input df doesn't match expected format\"\n",
    "    df.iloc[1, joinID_loc] = joinid\n",
    "    joinid += 1\n",
    "    isContinue = False\n",
    "#     display(df.head(20))\n",
    "    for index, row in df.iloc[2:].iterrows():\n",
    "        # do something\n",
    "        if isContinue and isNativeRow(row['name']):\n",
    "            joinid += 1\n",
    "            isContinue = False\n",
    "            \n",
    "        df.loc[index, 'joinID'] = joinid\n",
    "        \n",
    "        if isNativeRow(row['name']) or isfixIndexJoinID(index):\n",
    "            joinid += 1\n",
    "        elif not isContinue:\n",
    "            isContinue = True\n",
    "        \n",
    "    df['joinID'] = df['joinID'].astype(int)\n",
    "\n",
    "addJoinIDs(b2df)\n",
    "display(b2df.tail(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>duration</th>\n",
       "      <th>registersPerThread</th>\n",
       "      <th>staticSMem</th>\n",
       "      <th>dynamicSMem</th>\n",
       "      <th>size</th>\n",
       "      <th>throughput</th>\n",
       "      <th>context</th>\n",
       "      <th>stream</th>\n",
       "      <th>name</th>\n",
       "      <th>corrid</th>\n",
       "      <th>joinID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>92025</td>\n",
       "      <td>1.089080e+07</td>\n",
       "      <td>311.326</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.343750</td>\n",
       "      <td>7.351838</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>[CUDA memcpy HtoD]</td>\n",
       "      <td>84083</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92054</td>\n",
       "      <td>1.089151e+07</td>\n",
       "      <td>1.664</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.008955</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>[CUDA memcpy HtoD]</td>\n",
       "      <td>84108</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92104</td>\n",
       "      <td>1.089177e+07</td>\n",
       "      <td>2.464</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>cudnn::gemm::computeOffsetsKernel(cudnn::gemm::ComputeOffsetsParams)</td>\n",
       "      <td>84158</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92110</td>\n",
       "      <td>1.089180e+07</td>\n",
       "      <td>118.784</td>\n",
       "      <td>128.0</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>volta_scudnn_128x64_relu_medium_nn_v1</td>\n",
       "      <td>84161</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92141</td>\n",
       "      <td>1.089194e+07</td>\n",
       "      <td>3.008</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>void at::native::elementwise_kernel&lt;128, 4, at::native::gpu_kernel_impl&lt;at::native::gpu_kernel_with_scalars&lt;at::native::add_kernel_cuda(at::TensorIterator&amp;, c10::Scalar)::{lambda()#1}::operator()(...</td>\n",
       "      <td>84189</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92229</td>\n",
       "      <td>1.089224e+07</td>\n",
       "      <td>232.127</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.140625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>void cudnn::detail::bn_fw_tr_1C11_kernel_NCHW&lt;float, float, int=512, bool=1, int=1&gt;(cudnnTensorStruct, float const *, cudnn::detail::bn_fw_tr_1C11_kernel_NCHW&lt;float, float, int=512, bool=1, int=1&gt;...</td>\n",
       "      <td>84279</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92291</td>\n",
       "      <td>1.089248e+07</td>\n",
       "      <td>32.863</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>void at::native::elementwise_kernel&lt;512, 1, at::native::gpu_kernel_impl&lt;at::native::threshold_kernel_impl&lt;float&gt;(at::TensorIterator&amp;, float, float)::{lambda(float, float)#1}&gt;(at::TensorIterator&amp;, ...</td>\n",
       "      <td>84303</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92301</td>\n",
       "      <td>1.089253e+07</td>\n",
       "      <td>33.632</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>void at::native::_GLOBAL__N__63_tmpxft_00008bdb_00000000_11_DilatedMaxPool2d_compute_75_cpp1_ii_db999de0::MaxPoolForward&lt;float, float&gt;(int, float const *, int, int, int, int, int, int, int, int, i...</td>\n",
       "      <td>84341</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92350</td>\n",
       "      <td>1.089303e+07</td>\n",
       "      <td>1.920</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>cudnn::gemm::computeOffsetsKernel(cudnn::gemm::ComputeOffsetsParams)</td>\n",
       "      <td>84397</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92355</td>\n",
       "      <td>1.089305e+07</td>\n",
       "      <td>282.975</td>\n",
       "      <td>128.0</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>volta_scudnn_128x64_relu_interior_nn_v1</td>\n",
       "      <td>84400</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92440</td>\n",
       "      <td>1.089335e+07</td>\n",
       "      <td>2.368</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>void at::native::elementwise_kernel&lt;128, 4, at::native::gpu_kernel_impl&lt;at::native::gpu_kernel_with_scalars&lt;at::native::add_kernel_cuda(at::TensorIterator&amp;, c10::Scalar)::{lambda()#1}::operator()(...</td>\n",
       "      <td>84428</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92474</td>\n",
       "      <td>1.089374e+07</td>\n",
       "      <td>414.269</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.140625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>void cudnn::detail::bn_fw_tr_1C11_kernel_NCHW&lt;float, float, int=512, bool=1, int=1&gt;(cudnnTensorStruct, float const *, cudnn::detail::bn_fw_tr_1C11_kernel_NCHW&lt;float, float, int=512, bool=1, int=1&gt;...</td>\n",
       "      <td>84519</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92541</td>\n",
       "      <td>1.089417e+07</td>\n",
       "      <td>200.031</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>void at::native::elementwise_kernel&lt;512, 1, at::native::gpu_kernel_impl&lt;at::native::threshold_kernel_impl&lt;float&gt;(at::TensorIterator&amp;, float, float)::{lambda(float, float)#1}&gt;(at::TensorIterator&amp;, ...</td>\n",
       "      <td>84543</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92563</td>\n",
       "      <td>1.089473e+07</td>\n",
       "      <td>339.294</td>\n",
       "      <td>63.0</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>void cudnn::detail::implicit_convolve_sgemm&lt;float, float, int=1024, int=5, int=5, int=3, int=3, int=3, int=1, bool=1, bool=0, bool=1&gt;(int, int, int, float const *, int, float*, cudnn::detail::impl...</td>\n",
       "      <td>84604</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92568</td>\n",
       "      <td>1.089476e+07</td>\n",
       "      <td>319.518</td>\n",
       "      <td>63.0</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>void cudnn::detail::implicit_convolve_sgemm&lt;float, float, int=1024, int=5, int=5, int=3, int=3, int=3, int=1, bool=1, bool=0, bool=1&gt;(int, int, int, float const *, int, float*, cudnn::detail::impl...</td>\n",
       "      <td>84608</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92577</td>\n",
       "      <td>1.089483e+07</td>\n",
       "      <td>361.982</td>\n",
       "      <td>63.0</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>void cudnn::detail::implicit_convolve_sgemm&lt;float, float, int=1024, int=5, int=5, int=3, int=3, int=3, int=1, bool=1, bool=0, bool=1&gt;(int, int, int, float const *, int, float*, cudnn::detail::impl...</td>\n",
       "      <td>84612</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92594</td>\n",
       "      <td>1.089493e+07</td>\n",
       "      <td>360.574</td>\n",
       "      <td>63.0</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>void cudnn::detail::implicit_convolve_sgemm&lt;float, float, int=1024, int=5, int=5, int=3, int=3, int=3, int=1, bool=1, bool=0, bool=1&gt;(int, int, int, float const *, int, float*, cudnn::detail::impl...</td>\n",
       "      <td>84616</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92609</td>\n",
       "      <td>1.089502e+07</td>\n",
       "      <td>374.206</td>\n",
       "      <td>63.0</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>void cudnn::detail::implicit_convolve_sgemm&lt;float, float, int=1024, int=5, int=5, int=3, int=3, int=3, int=1, bool=1, bool=0, bool=1&gt;(int, int, int, float const *, int, float*, cudnn::detail::impl...</td>\n",
       "      <td>84620</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92624</td>\n",
       "      <td>1.089512e+07</td>\n",
       "      <td>372.510</td>\n",
       "      <td>63.0</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>void cudnn::detail::implicit_convolve_sgemm&lt;float, float, int=1024, int=5, int=5, int=3, int=3, int=3, int=1, bool=1, bool=0, bool=1&gt;(int, int, int, float const *, int, float*, cudnn::detail::impl...</td>\n",
       "      <td>84624</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92641</td>\n",
       "      <td>1.089522e+07</td>\n",
       "      <td>342.014</td>\n",
       "      <td>63.0</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>void cudnn::detail::implicit_convolve_sgemm&lt;float, float, int=1024, int=5, int=5, int=3, int=3, int=3, int=1, bool=1, bool=0, bool=1&gt;(int, int, int, float const *, int, float*, cudnn::detail::impl...</td>\n",
       "      <td>84628</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92656</td>\n",
       "      <td>1.089532e+07</td>\n",
       "      <td>382.974</td>\n",
       "      <td>63.0</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>void cudnn::detail::implicit_convolve_sgemm&lt;float, float, int=1024, int=5, int=5, int=3, int=3, int=3, int=1, bool=1, bool=0, bool=1&gt;(int, int, int, float const *, int, float*, cudnn::detail::impl...</td>\n",
       "      <td>84632</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92671</td>\n",
       "      <td>1.089541e+07</td>\n",
       "      <td>379.006</td>\n",
       "      <td>63.0</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>void cudnn::detail::implicit_convolve_sgemm&lt;float, float, int=1024, int=5, int=5, int=3, int=3, int=3, int=1, bool=1, bool=0, bool=1&gt;(int, int, int, float const *, int, float*, cudnn::detail::impl...</td>\n",
       "      <td>84636</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92687</td>\n",
       "      <td>1.089551e+07</td>\n",
       "      <td>361.598</td>\n",
       "      <td>63.0</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>void cudnn::detail::implicit_convolve_sgemm&lt;float, float, int=1024, int=5, int=5, int=3, int=3, int=3, int=1, bool=1, bool=0, bool=1&gt;(int, int, int, float const *, int, float*, cudnn::detail::impl...</td>\n",
       "      <td>84640</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92715</td>\n",
       "      <td>1.089561e+07</td>\n",
       "      <td>381.566</td>\n",
       "      <td>63.0</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>void cudnn::detail::implicit_convolve_sgemm&lt;float, float, int=1024, int=5, int=5, int=3, int=3, int=3, int=1, bool=1, bool=0, bool=1&gt;(int, int, int, float const *, int, float*, cudnn::detail::impl...</td>\n",
       "      <td>84644</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92735</td>\n",
       "      <td>1.089570e+07</td>\n",
       "      <td>351.486</td>\n",
       "      <td>63.0</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>void cudnn::detail::implicit_convolve_sgemm&lt;float, float, int=1024, int=5, int=5, int=3, int=3, int=3, int=1, bool=1, bool=0, bool=1&gt;(int, int, int, float const *, int, float*, cudnn::detail::impl...</td>\n",
       "      <td>84648</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92752</td>\n",
       "      <td>1.089580e+07</td>\n",
       "      <td>377.502</td>\n",
       "      <td>63.0</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>void cudnn::detail::implicit_convolve_sgemm&lt;float, float, int=1024, int=5, int=5, int=3, int=3, int=3, int=1, bool=1, bool=0, bool=1&gt;(int, int, int, float const *, int, float*, cudnn::detail::impl...</td>\n",
       "      <td>84652</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92795</td>\n",
       "      <td>1.089590e+07</td>\n",
       "      <td>379.901</td>\n",
       "      <td>63.0</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>void cudnn::detail::implicit_convolve_sgemm&lt;float, float, int=1024, int=5, int=5, int=3, int=3, int=3, int=1, bool=1, bool=0, bool=1&gt;(int, int, int, float const *, int, float*, cudnn::detail::impl...</td>\n",
       "      <td>84656</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92820</td>\n",
       "      <td>1.089600e+07</td>\n",
       "      <td>381.854</td>\n",
       "      <td>63.0</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>void cudnn::detail::implicit_convolve_sgemm&lt;float, float, int=1024, int=5, int=5, int=3, int=3, int=3, int=1, bool=1, bool=0, bool=1&gt;(int, int, int, float const *, int, float*, cudnn::detail::impl...</td>\n",
       "      <td>84660</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92821</td>\n",
       "      <td>1.089610e+07</td>\n",
       "      <td>383.646</td>\n",
       "      <td>63.0</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>void cudnn::detail::implicit_convolve_sgemm&lt;float, float, int=1024, int=5, int=5, int=3, int=3, int=3, int=1, bool=1, bool=0, bool=1&gt;(int, int, int, float const *, int, float*, cudnn::detail::impl...</td>\n",
       "      <td>84664</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92822</td>\n",
       "      <td>1.089619e+07</td>\n",
       "      <td>363.518</td>\n",
       "      <td>63.0</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>void cudnn::detail::implicit_convolve_sgemm&lt;float, float, int=1024, int=5, int=5, int=3, int=3, int=3, int=1, bool=1, bool=0, bool=1&gt;(int, int, int, float const *, int, float*, cudnn::detail::impl...</td>\n",
       "      <td>84668</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92835</td>\n",
       "      <td>1.089629e+07</td>\n",
       "      <td>371.166</td>\n",
       "      <td>63.0</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>void cudnn::detail::implicit_convolve_sgemm&lt;float, float, int=1024, int=5, int=5, int=3, int=3, int=3, int=1, bool=1, bool=0, bool=1&gt;(int, int, int, float const *, int, float*, cudnn::detail::impl...</td>\n",
       "      <td>84672</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92853</td>\n",
       "      <td>1.089639e+07</td>\n",
       "      <td>368.606</td>\n",
       "      <td>63.0</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>void cudnn::detail::implicit_convolve_sgemm&lt;float, float, int=1024, int=5, int=5, int=3, int=3, int=3, int=1, bool=1, bool=0, bool=1&gt;(int, int, int, float const *, int, float*, cudnn::detail::impl...</td>\n",
       "      <td>84676</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92872</td>\n",
       "      <td>1.089649e+07</td>\n",
       "      <td>345.310</td>\n",
       "      <td>63.0</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>void cudnn::detail::implicit_convolve_sgemm&lt;float, float, int=1024, int=5, int=5, int=3, int=3, int=3, int=1, bool=1, bool=0, bool=1&gt;(int, int, int, float const *, int, float*, cudnn::detail::impl...</td>\n",
       "      <td>84680</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92906</td>\n",
       "      <td>1.089659e+07</td>\n",
       "      <td>373.662</td>\n",
       "      <td>63.0</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>void cudnn::detail::implicit_convolve_sgemm&lt;float, float, int=1024, int=5, int=5, int=3, int=3, int=3, int=1, bool=1, bool=0, bool=1&gt;(int, int, int, float const *, int, float*, cudnn::detail::impl...</td>\n",
       "      <td>84684</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92922</td>\n",
       "      <td>1.089668e+07</td>\n",
       "      <td>375.838</td>\n",
       "      <td>63.0</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>void cudnn::detail::implicit_convolve_sgemm&lt;float, float, int=1024, int=5, int=5, int=3, int=3, int=3, int=1, bool=1, bool=0, bool=1&gt;(int, int, int, float const *, int, float*, cudnn::detail::impl...</td>\n",
       "      <td>84688</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92946</td>\n",
       "      <td>1.089678e+07</td>\n",
       "      <td>379.133</td>\n",
       "      <td>63.0</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>void cudnn::detail::implicit_convolve_sgemm&lt;float, float, int=1024, int=5, int=5, int=3, int=3, int=3, int=1, bool=1, bool=0, bool=1&gt;(int, int, int, float const *, int, float*, cudnn::detail::impl...</td>\n",
       "      <td>84692</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92964</td>\n",
       "      <td>1.089688e+07</td>\n",
       "      <td>372.478</td>\n",
       "      <td>63.0</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>void cudnn::detail::implicit_convolve_sgemm&lt;float, float, int=1024, int=5, int=5, int=3, int=3, int=3, int=1, bool=1, bool=0, bool=1&gt;(int, int, int, float const *, int, float*, cudnn::detail::impl...</td>\n",
       "      <td>84696</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93003</td>\n",
       "      <td>1.089698e+07</td>\n",
       "      <td>362.205</td>\n",
       "      <td>63.0</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>void cudnn::detail::implicit_convolve_sgemm&lt;float, float, int=1024, int=5, int=5, int=3, int=3, int=3, int=1, bool=1, bool=0, bool=1&gt;(int, int, int, float const *, int, float*, cudnn::detail::impl...</td>\n",
       "      <td>84700</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93036</td>\n",
       "      <td>1.089707e+07</td>\n",
       "      <td>360.510</td>\n",
       "      <td>63.0</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>void cudnn::detail::implicit_convolve_sgemm&lt;float, float, int=1024, int=5, int=5, int=3, int=3, int=3, int=1, bool=1, bool=0, bool=1&gt;(int, int, int, float const *, int, float*, cudnn::detail::impl...</td>\n",
       "      <td>84704</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93057</td>\n",
       "      <td>1.089717e+07</td>\n",
       "      <td>371.518</td>\n",
       "      <td>63.0</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>void cudnn::detail::implicit_convolve_sgemm&lt;float, float, int=1024, int=5, int=5, int=3, int=3, int=3, int=1, bool=1, bool=0, bool=1&gt;(int, int, int, float const *, int, float*, cudnn::detail::impl...</td>\n",
       "      <td>84708</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93089</td>\n",
       "      <td>1.089727e+07</td>\n",
       "      <td>376.190</td>\n",
       "      <td>63.0</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>void cudnn::detail::implicit_convolve_sgemm&lt;float, float, int=1024, int=5, int=5, int=3, int=3, int=3, int=1, bool=1, bool=0, bool=1&gt;(int, int, int, float const *, int, float*, cudnn::detail::impl...</td>\n",
       "      <td>84712</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93108</td>\n",
       "      <td>1.089737e+07</td>\n",
       "      <td>366.206</td>\n",
       "      <td>63.0</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>void cudnn::detail::implicit_convolve_sgemm&lt;float, float, int=1024, int=5, int=5, int=3, int=3, int=3, int=1, bool=1, bool=0, bool=1&gt;(int, int, int, float const *, int, float*, cudnn::detail::impl...</td>\n",
       "      <td>84716</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93132</td>\n",
       "      <td>1.089746e+07</td>\n",
       "      <td>345.950</td>\n",
       "      <td>63.0</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>void cudnn::detail::implicit_convolve_sgemm&lt;float, float, int=1024, int=5, int=5, int=3, int=3, int=3, int=1, bool=1, bool=0, bool=1&gt;(int, int, int, float const *, int, float*, cudnn::detail::impl...</td>\n",
       "      <td>84720</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93149</td>\n",
       "      <td>1.089756e+07</td>\n",
       "      <td>319.902</td>\n",
       "      <td>63.0</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>void cudnn::detail::implicit_convolve_sgemm&lt;float, float, int=1024, int=5, int=5, int=3, int=3, int=3, int=1, bool=1, bool=0, bool=1&gt;(int, int, int, float const *, int, float*, cudnn::detail::impl...</td>\n",
       "      <td>84724</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93197</td>\n",
       "      <td>1.089766e+07</td>\n",
       "      <td>248.894</td>\n",
       "      <td>63.0</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>void cudnn::detail::implicit_convolve_sgemm&lt;float, float, int=1024, int=5, int=5, int=3, int=3, int=3, int=1, bool=1, bool=0, bool=1&gt;(int, int, int, float const *, int, float*, cudnn::detail::impl...</td>\n",
       "      <td>84728</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93265</td>\n",
       "      <td>1.089793e+07</td>\n",
       "      <td>2.400</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>void at::native::elementwise_kernel&lt;128, 4, at::native::gpu_kernel_impl&lt;at::native::gpu_kernel_with_scalars&lt;at::native::add_kernel_cuda(at::TensorIterator&amp;, c10::Scalar)::{lambda()#1}::operator()(...</td>\n",
       "      <td>84773</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93273</td>\n",
       "      <td>1.089798e+07</td>\n",
       "      <td>415.134</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.140625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>void cudnn::detail::bn_fw_tr_1C11_kernel_NCHW&lt;float, float, int=512, bool=1, int=1&gt;(cudnnTensorStruct, float const *, cudnn::detail::bn_fw_tr_1C11_kernel_NCHW&lt;float, float, int=512, bool=1, int=1&gt;...</td>\n",
       "      <td>84864</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93318</td>\n",
       "      <td>1.089840e+07</td>\n",
       "      <td>200.126</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>void at::native::elementwise_kernel&lt;512, 1, at::native::gpu_kernel_impl&lt;at::native::threshold_kernel_impl&lt;float&gt;(at::TensorIterator&amp;, float, float)::{lambda(float, float)#1}&gt;(at::TensorIterator&amp;, ...</td>\n",
       "      <td>84888</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93357</td>\n",
       "      <td>1.089863e+07</td>\n",
       "      <td>1.952</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>cudnn::gemm::computeOffsetsKernel(cudnn::gemm::ComputeOffsetsParams)</td>\n",
       "      <td>84939</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93358</td>\n",
       "      <td>1.089863e+07</td>\n",
       "      <td>1315.833</td>\n",
       "      <td>128.0</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>volta_scudnn_128x64_relu_interior_nn_v1</td>\n",
       "      <td>84942</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              start  duration  registersPerThread  staticSMem  dynamicSMem  \\\n",
       "92025  1.089080e+07   311.326                 NaN         NaN          NaN   \n",
       "92054  1.089151e+07     1.664                 NaN         NaN          NaN   \n",
       "92104  1.089177e+07     2.464                16.0    0.000000          0.0   \n",
       "92110  1.089180e+07   118.784               128.0   16.000000          0.0   \n",
       "92141  1.089194e+07     3.008                16.0    0.000000          0.0   \n",
       "92229  1.089224e+07   232.127                32.0    0.140625          0.0   \n",
       "92291  1.089248e+07    32.863                16.0    0.000000          0.0   \n",
       "92301  1.089253e+07    33.632                24.0    0.000000          0.0   \n",
       "92350  1.089303e+07     1.920                16.0    0.000000          0.0   \n",
       "92355  1.089305e+07   282.975               128.0   16.000000          0.0   \n",
       "92440  1.089335e+07     2.368                16.0    0.000000          0.0   \n",
       "92474  1.089374e+07   414.269                32.0    0.140625          0.0   \n",
       "92541  1.089417e+07   200.031                16.0    0.000000          0.0   \n",
       "92563  1.089473e+07   339.294                63.0    2.250000          0.0   \n",
       "92568  1.089476e+07   319.518                63.0    2.250000          0.0   \n",
       "92577  1.089483e+07   361.982                63.0    2.250000          0.0   \n",
       "92594  1.089493e+07   360.574                63.0    2.250000          0.0   \n",
       "92609  1.089502e+07   374.206                63.0    2.250000          0.0   \n",
       "92624  1.089512e+07   372.510                63.0    2.250000          0.0   \n",
       "92641  1.089522e+07   342.014                63.0    2.250000          0.0   \n",
       "92656  1.089532e+07   382.974                63.0    2.250000          0.0   \n",
       "92671  1.089541e+07   379.006                63.0    2.250000          0.0   \n",
       "92687  1.089551e+07   361.598                63.0    2.250000          0.0   \n",
       "92715  1.089561e+07   381.566                63.0    2.250000          0.0   \n",
       "92735  1.089570e+07   351.486                63.0    2.250000          0.0   \n",
       "92752  1.089580e+07   377.502                63.0    2.250000          0.0   \n",
       "92795  1.089590e+07   379.901                63.0    2.250000          0.0   \n",
       "92820  1.089600e+07   381.854                63.0    2.250000          0.0   \n",
       "92821  1.089610e+07   383.646                63.0    2.250000          0.0   \n",
       "92822  1.089619e+07   363.518                63.0    2.250000          0.0   \n",
       "92835  1.089629e+07   371.166                63.0    2.250000          0.0   \n",
       "92853  1.089639e+07   368.606                63.0    2.250000          0.0   \n",
       "92872  1.089649e+07   345.310                63.0    2.250000          0.0   \n",
       "92906  1.089659e+07   373.662                63.0    2.250000          0.0   \n",
       "92922  1.089668e+07   375.838                63.0    2.250000          0.0   \n",
       "92946  1.089678e+07   379.133                63.0    2.250000          0.0   \n",
       "92964  1.089688e+07   372.478                63.0    2.250000          0.0   \n",
       "93003  1.089698e+07   362.205                63.0    2.250000          0.0   \n",
       "93036  1.089707e+07   360.510                63.0    2.250000          0.0   \n",
       "93057  1.089717e+07   371.518                63.0    2.250000          0.0   \n",
       "93089  1.089727e+07   376.190                63.0    2.250000          0.0   \n",
       "93108  1.089737e+07   366.206                63.0    2.250000          0.0   \n",
       "93132  1.089746e+07   345.950                63.0    2.250000          0.0   \n",
       "93149  1.089756e+07   319.902                63.0    2.250000          0.0   \n",
       "93197  1.089766e+07   248.894                63.0    2.250000          0.0   \n",
       "93265  1.089793e+07     2.400                16.0    0.000000          0.0   \n",
       "93273  1.089798e+07   415.134                32.0    0.140625          0.0   \n",
       "93318  1.089840e+07   200.126                16.0    0.000000          0.0   \n",
       "93357  1.089863e+07     1.952                16.0    0.000000          0.0   \n",
       "93358  1.089863e+07  1315.833               128.0   16.000000          0.0   \n",
       "\n",
       "           size  throughput  context  stream  \\\n",
       "92025  2.343750    7.351838      1.0     7.0   \n",
       "92054  0.000015    0.008955      1.0     7.0   \n",
       "92104       NaN         NaN      1.0     7.0   \n",
       "92110       NaN         NaN      1.0     7.0   \n",
       "92141       NaN         NaN      1.0     7.0   \n",
       "92229       NaN         NaN      1.0     7.0   \n",
       "92291       NaN         NaN      1.0     7.0   \n",
       "92301       NaN         NaN      1.0     7.0   \n",
       "92350       NaN         NaN      1.0     7.0   \n",
       "92355       NaN         NaN      1.0     7.0   \n",
       "92440       NaN         NaN      1.0     7.0   \n",
       "92474       NaN         NaN      1.0     7.0   \n",
       "92541       NaN         NaN      1.0     7.0   \n",
       "92563       NaN         NaN      1.0    14.0   \n",
       "92568       NaN         NaN      1.0    15.0   \n",
       "92577       NaN         NaN      1.0    16.0   \n",
       "92594       NaN         NaN      1.0    17.0   \n",
       "92609       NaN         NaN      1.0    18.0   \n",
       "92624       NaN         NaN      1.0    19.0   \n",
       "92641       NaN         NaN      1.0    20.0   \n",
       "92656       NaN         NaN      1.0    21.0   \n",
       "92671       NaN         NaN      1.0    14.0   \n",
       "92687       NaN         NaN      1.0    15.0   \n",
       "92715       NaN         NaN      1.0    16.0   \n",
       "92735       NaN         NaN      1.0    17.0   \n",
       "92752       NaN         NaN      1.0    18.0   \n",
       "92795       NaN         NaN      1.0    19.0   \n",
       "92820       NaN         NaN      1.0    20.0   \n",
       "92821       NaN         NaN      1.0    21.0   \n",
       "92822       NaN         NaN      1.0    14.0   \n",
       "92835       NaN         NaN      1.0    15.0   \n",
       "92853       NaN         NaN      1.0    16.0   \n",
       "92872       NaN         NaN      1.0    17.0   \n",
       "92906       NaN         NaN      1.0    18.0   \n",
       "92922       NaN         NaN      1.0    19.0   \n",
       "92946       NaN         NaN      1.0    20.0   \n",
       "92964       NaN         NaN      1.0    21.0   \n",
       "93003       NaN         NaN      1.0    14.0   \n",
       "93036       NaN         NaN      1.0    15.0   \n",
       "93057       NaN         NaN      1.0    16.0   \n",
       "93089       NaN         NaN      1.0    17.0   \n",
       "93108       NaN         NaN      1.0    18.0   \n",
       "93132       NaN         NaN      1.0    19.0   \n",
       "93149       NaN         NaN      1.0    20.0   \n",
       "93197       NaN         NaN      1.0    21.0   \n",
       "93265       NaN         NaN      1.0     7.0   \n",
       "93273       NaN         NaN      1.0     7.0   \n",
       "93318       NaN         NaN      1.0     7.0   \n",
       "93357       NaN         NaN      1.0     7.0   \n",
       "93358       NaN         NaN      1.0     7.0   \n",
       "\n",
       "                                                                                                                                                                                                          name  \\\n",
       "92025                                                                                                                                                                                       [CUDA memcpy HtoD]   \n",
       "92054                                                                                                                                                                                       [CUDA memcpy HtoD]   \n",
       "92104                                                                                                                                     cudnn::gemm::computeOffsetsKernel(cudnn::gemm::ComputeOffsetsParams)   \n",
       "92110                                                                                                                                                                    volta_scudnn_128x64_relu_medium_nn_v1   \n",
       "92141  void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl<at::native::gpu_kernel_with_scalars<at::native::add_kernel_cuda(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()(...   \n",
       "92229  void cudnn::detail::bn_fw_tr_1C11_kernel_NCHW<float, float, int=512, bool=1, int=1>(cudnnTensorStruct, float const *, cudnn::detail::bn_fw_tr_1C11_kernel_NCHW<float, float, int=512, bool=1, int=1>...   \n",
       "92291  void at::native::elementwise_kernel<512, 1, at::native::gpu_kernel_impl<at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}>(at::TensorIterator&, ...   \n",
       "92301  void at::native::_GLOBAL__N__63_tmpxft_00008bdb_00000000_11_DilatedMaxPool2d_compute_75_cpp1_ii_db999de0::MaxPoolForward<float, float>(int, float const *, int, int, int, int, int, int, int, int, i...   \n",
       "92350                                                                                                                                     cudnn::gemm::computeOffsetsKernel(cudnn::gemm::ComputeOffsetsParams)   \n",
       "92355                                                                                                                                                                  volta_scudnn_128x64_relu_interior_nn_v1   \n",
       "92440  void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl<at::native::gpu_kernel_with_scalars<at::native::add_kernel_cuda(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()(...   \n",
       "92474  void cudnn::detail::bn_fw_tr_1C11_kernel_NCHW<float, float, int=512, bool=1, int=1>(cudnnTensorStruct, float const *, cudnn::detail::bn_fw_tr_1C11_kernel_NCHW<float, float, int=512, bool=1, int=1>...   \n",
       "92541  void at::native::elementwise_kernel<512, 1, at::native::gpu_kernel_impl<at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}>(at::TensorIterator&, ...   \n",
       "92563  void cudnn::detail::implicit_convolve_sgemm<float, float, int=1024, int=5, int=5, int=3, int=3, int=3, int=1, bool=1, bool=0, bool=1>(int, int, int, float const *, int, float*, cudnn::detail::impl...   \n",
       "92568  void cudnn::detail::implicit_convolve_sgemm<float, float, int=1024, int=5, int=5, int=3, int=3, int=3, int=1, bool=1, bool=0, bool=1>(int, int, int, float const *, int, float*, cudnn::detail::impl...   \n",
       "92577  void cudnn::detail::implicit_convolve_sgemm<float, float, int=1024, int=5, int=5, int=3, int=3, int=3, int=1, bool=1, bool=0, bool=1>(int, int, int, float const *, int, float*, cudnn::detail::impl...   \n",
       "92594  void cudnn::detail::implicit_convolve_sgemm<float, float, int=1024, int=5, int=5, int=3, int=3, int=3, int=1, bool=1, bool=0, bool=1>(int, int, int, float const *, int, float*, cudnn::detail::impl...   \n",
       "92609  void cudnn::detail::implicit_convolve_sgemm<float, float, int=1024, int=5, int=5, int=3, int=3, int=3, int=1, bool=1, bool=0, bool=1>(int, int, int, float const *, int, float*, cudnn::detail::impl...   \n",
       "92624  void cudnn::detail::implicit_convolve_sgemm<float, float, int=1024, int=5, int=5, int=3, int=3, int=3, int=1, bool=1, bool=0, bool=1>(int, int, int, float const *, int, float*, cudnn::detail::impl...   \n",
       "92641  void cudnn::detail::implicit_convolve_sgemm<float, float, int=1024, int=5, int=5, int=3, int=3, int=3, int=1, bool=1, bool=0, bool=1>(int, int, int, float const *, int, float*, cudnn::detail::impl...   \n",
       "92656  void cudnn::detail::implicit_convolve_sgemm<float, float, int=1024, int=5, int=5, int=3, int=3, int=3, int=1, bool=1, bool=0, bool=1>(int, int, int, float const *, int, float*, cudnn::detail::impl...   \n",
       "92671  void cudnn::detail::implicit_convolve_sgemm<float, float, int=1024, int=5, int=5, int=3, int=3, int=3, int=1, bool=1, bool=0, bool=1>(int, int, int, float const *, int, float*, cudnn::detail::impl...   \n",
       "92687  void cudnn::detail::implicit_convolve_sgemm<float, float, int=1024, int=5, int=5, int=3, int=3, int=3, int=1, bool=1, bool=0, bool=1>(int, int, int, float const *, int, float*, cudnn::detail::impl...   \n",
       "92715  void cudnn::detail::implicit_convolve_sgemm<float, float, int=1024, int=5, int=5, int=3, int=3, int=3, int=1, bool=1, bool=0, bool=1>(int, int, int, float const *, int, float*, cudnn::detail::impl...   \n",
       "92735  void cudnn::detail::implicit_convolve_sgemm<float, float, int=1024, int=5, int=5, int=3, int=3, int=3, int=1, bool=1, bool=0, bool=1>(int, int, int, float const *, int, float*, cudnn::detail::impl...   \n",
       "92752  void cudnn::detail::implicit_convolve_sgemm<float, float, int=1024, int=5, int=5, int=3, int=3, int=3, int=1, bool=1, bool=0, bool=1>(int, int, int, float const *, int, float*, cudnn::detail::impl...   \n",
       "92795  void cudnn::detail::implicit_convolve_sgemm<float, float, int=1024, int=5, int=5, int=3, int=3, int=3, int=1, bool=1, bool=0, bool=1>(int, int, int, float const *, int, float*, cudnn::detail::impl...   \n",
       "92820  void cudnn::detail::implicit_convolve_sgemm<float, float, int=1024, int=5, int=5, int=3, int=3, int=3, int=1, bool=1, bool=0, bool=1>(int, int, int, float const *, int, float*, cudnn::detail::impl...   \n",
       "92821  void cudnn::detail::implicit_convolve_sgemm<float, float, int=1024, int=5, int=5, int=3, int=3, int=3, int=1, bool=1, bool=0, bool=1>(int, int, int, float const *, int, float*, cudnn::detail::impl...   \n",
       "92822  void cudnn::detail::implicit_convolve_sgemm<float, float, int=1024, int=5, int=5, int=3, int=3, int=3, int=1, bool=1, bool=0, bool=1>(int, int, int, float const *, int, float*, cudnn::detail::impl...   \n",
       "92835  void cudnn::detail::implicit_convolve_sgemm<float, float, int=1024, int=5, int=5, int=3, int=3, int=3, int=1, bool=1, bool=0, bool=1>(int, int, int, float const *, int, float*, cudnn::detail::impl...   \n",
       "92853  void cudnn::detail::implicit_convolve_sgemm<float, float, int=1024, int=5, int=5, int=3, int=3, int=3, int=1, bool=1, bool=0, bool=1>(int, int, int, float const *, int, float*, cudnn::detail::impl...   \n",
       "92872  void cudnn::detail::implicit_convolve_sgemm<float, float, int=1024, int=5, int=5, int=3, int=3, int=3, int=1, bool=1, bool=0, bool=1>(int, int, int, float const *, int, float*, cudnn::detail::impl...   \n",
       "92906  void cudnn::detail::implicit_convolve_sgemm<float, float, int=1024, int=5, int=5, int=3, int=3, int=3, int=1, bool=1, bool=0, bool=1>(int, int, int, float const *, int, float*, cudnn::detail::impl...   \n",
       "92922  void cudnn::detail::implicit_convolve_sgemm<float, float, int=1024, int=5, int=5, int=3, int=3, int=3, int=1, bool=1, bool=0, bool=1>(int, int, int, float const *, int, float*, cudnn::detail::impl...   \n",
       "92946  void cudnn::detail::implicit_convolve_sgemm<float, float, int=1024, int=5, int=5, int=3, int=3, int=3, int=1, bool=1, bool=0, bool=1>(int, int, int, float const *, int, float*, cudnn::detail::impl...   \n",
       "92964  void cudnn::detail::implicit_convolve_sgemm<float, float, int=1024, int=5, int=5, int=3, int=3, int=3, int=1, bool=1, bool=0, bool=1>(int, int, int, float const *, int, float*, cudnn::detail::impl...   \n",
       "93003  void cudnn::detail::implicit_convolve_sgemm<float, float, int=1024, int=5, int=5, int=3, int=3, int=3, int=1, bool=1, bool=0, bool=1>(int, int, int, float const *, int, float*, cudnn::detail::impl...   \n",
       "93036  void cudnn::detail::implicit_convolve_sgemm<float, float, int=1024, int=5, int=5, int=3, int=3, int=3, int=1, bool=1, bool=0, bool=1>(int, int, int, float const *, int, float*, cudnn::detail::impl...   \n",
       "93057  void cudnn::detail::implicit_convolve_sgemm<float, float, int=1024, int=5, int=5, int=3, int=3, int=3, int=1, bool=1, bool=0, bool=1>(int, int, int, float const *, int, float*, cudnn::detail::impl...   \n",
       "93089  void cudnn::detail::implicit_convolve_sgemm<float, float, int=1024, int=5, int=5, int=3, int=3, int=3, int=1, bool=1, bool=0, bool=1>(int, int, int, float const *, int, float*, cudnn::detail::impl...   \n",
       "93108  void cudnn::detail::implicit_convolve_sgemm<float, float, int=1024, int=5, int=5, int=3, int=3, int=3, int=1, bool=1, bool=0, bool=1>(int, int, int, float const *, int, float*, cudnn::detail::impl...   \n",
       "93132  void cudnn::detail::implicit_convolve_sgemm<float, float, int=1024, int=5, int=5, int=3, int=3, int=3, int=1, bool=1, bool=0, bool=1>(int, int, int, float const *, int, float*, cudnn::detail::impl...   \n",
       "93149  void cudnn::detail::implicit_convolve_sgemm<float, float, int=1024, int=5, int=5, int=3, int=3, int=3, int=1, bool=1, bool=0, bool=1>(int, int, int, float const *, int, float*, cudnn::detail::impl...   \n",
       "93197  void cudnn::detail::implicit_convolve_sgemm<float, float, int=1024, int=5, int=5, int=3, int=3, int=3, int=1, bool=1, bool=0, bool=1>(int, int, int, float const *, int, float*, cudnn::detail::impl...   \n",
       "93265  void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl<at::native::gpu_kernel_with_scalars<at::native::add_kernel_cuda(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()(...   \n",
       "93273  void cudnn::detail::bn_fw_tr_1C11_kernel_NCHW<float, float, int=512, bool=1, int=1>(cudnnTensorStruct, float const *, cudnn::detail::bn_fw_tr_1C11_kernel_NCHW<float, float, int=512, bool=1, int=1>...   \n",
       "93318  void at::native::elementwise_kernel<512, 1, at::native::gpu_kernel_impl<at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}>(at::TensorIterator&, ...   \n",
       "93357                                                                                                                                     cudnn::gemm::computeOffsetsKernel(cudnn::gemm::ComputeOffsetsParams)   \n",
       "93358                                                                                                                                                                  volta_scudnn_128x64_relu_interior_nn_v1   \n",
       "\n",
       "       corrid  joinID  \n",
       "92025   84083       0  \n",
       "92054   84108       1  \n",
       "92104   84158       2  \n",
       "92110   84161       2  \n",
       "92141   84189       3  \n",
       "92229   84279       4  \n",
       "92291   84303       5  \n",
       "92301   84341       6  \n",
       "92350   84397       7  \n",
       "92355   84400       7  \n",
       "92440   84428       8  \n",
       "92474   84519       9  \n",
       "92541   84543      10  \n",
       "92563   84604      11  \n",
       "92568   84608      11  \n",
       "92577   84612      11  \n",
       "92594   84616      11  \n",
       "92609   84620      11  \n",
       "92624   84624      11  \n",
       "92641   84628      11  \n",
       "92656   84632      11  \n",
       "92671   84636      11  \n",
       "92687   84640      11  \n",
       "92715   84644      11  \n",
       "92735   84648      11  \n",
       "92752   84652      11  \n",
       "92795   84656      11  \n",
       "92820   84660      11  \n",
       "92821   84664      11  \n",
       "92822   84668      11  \n",
       "92835   84672      11  \n",
       "92853   84676      11  \n",
       "92872   84680      11  \n",
       "92906   84684      11  \n",
       "92922   84688      11  \n",
       "92946   84692      11  \n",
       "92964   84696      11  \n",
       "93003   84700      11  \n",
       "93036   84704      11  \n",
       "93057   84708      11  \n",
       "93089   84712      11  \n",
       "93108   84716      11  \n",
       "93132   84720      11  \n",
       "93149   84724      11  \n",
       "93197   84728      11  \n",
       "93265   84773      12  \n",
       "93273   84864      13  \n",
       "93318   84888      14  \n",
       "93357   84939      15  \n",
       "93358   84942      15  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "b2df['joinID'] = b2df['joinID'].astype(int)\n",
    "display(b2df.head(50))\n",
    "b2df.to_csv('/workspace/logs/tmp_b2df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1633"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(b2df.index[b2df['name'].str.contains(\"at::native::\")].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>duration</th>\n",
       "      <th>registersPerThread</th>\n",
       "      <th>staticSMem</th>\n",
       "      <th>dynamicSMem</th>\n",
       "      <th>size</th>\n",
       "      <th>throughput</th>\n",
       "      <th>context</th>\n",
       "      <th>stream</th>\n",
       "      <th>name</th>\n",
       "      <th>corrid</th>\n",
       "      <th>joinID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>122257</td>\n",
       "      <td>1.109490e+07</td>\n",
       "      <td>2.400</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>void at::native::elementwise_kernel&lt;128, 4, at::native::gpu_kernel_impl&lt;at::native::gpu_kernel_with_scalars&lt;at::native::add_kernel_cuda(at::TensorIterator&amp;, c10::Scalar)::{lambda()#1}::operator()(...</td>\n",
       "      <td>107740</td>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122278</td>\n",
       "      <td>1.109496e+07</td>\n",
       "      <td>204.607</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.140625</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>void cudnn::detail::bn_fw_tr_1C11_singleread&lt;float, int=512, bool=1, int=1, int=2, int=0&gt;(cudnnTensorStruct, float const *, cudnn::detail::bn_fw_tr_1C11_singleread&lt;float, int=512, bool=1, int=1, i...</td>\n",
       "      <td>107830</td>\n",
       "      <td>412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122350</td>\n",
       "      <td>1.109518e+07</td>\n",
       "      <td>25.504</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>void at::native::elementwise_kernel&lt;512, 1, at::native::gpu_kernel_impl&lt;at::native::threshold_kernel_impl&lt;float&gt;(at::TensorIterator&amp;, float, float)::{lambda(float, float)#1}&gt;(at::TensorIterator&amp;, ...</td>\n",
       "      <td>107854</td>\n",
       "      <td>413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122372</td>\n",
       "      <td>1.109524e+07</td>\n",
       "      <td>2214.228</td>\n",
       "      <td>63.0</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>void cudnn::detail::implicit_convolve_sgemm&lt;float, float, int=1024, int=5, int=5, int=3, int=3, int=3, int=1, bool=1, bool=0, bool=1&gt;(int, int, int, float const *, int, float*, cudnn::detail::impl...</td>\n",
       "      <td>107905</td>\n",
       "      <td>414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122412</td>\n",
       "      <td>1.109747e+07</td>\n",
       "      <td>2.368</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>void at::native::elementwise_kernel&lt;128, 4, at::native::gpu_kernel_impl&lt;at::native::gpu_kernel_with_scalars&lt;at::native::add_kernel_cuda(at::TensorIterator&amp;, c10::Scalar)::{lambda()#1}::operator()(...</td>\n",
       "      <td>107934</td>\n",
       "      <td>415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122429</td>\n",
       "      <td>1.109753e+07</td>\n",
       "      <td>36.127</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.140625</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>void cudnn::detail::bn_fw_tr_1C11_singleread&lt;float, int=512, bool=1, int=1, int=2, int=0&gt;(cudnnTensorStruct, float const *, cudnn::detail::bn_fw_tr_1C11_singleread&lt;float, int=512, bool=1, int=1, i...</td>\n",
       "      <td>108024</td>\n",
       "      <td>416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122458</td>\n",
       "      <td>1.109761e+07</td>\n",
       "      <td>197.823</td>\n",
       "      <td>63.0</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>void cudnn::detail::implicit_convolve_sgemm&lt;float, float, int=1024, int=5, int=5, int=3, int=3, int=3, int=1, bool=1, bool=0, bool=1&gt;(int, int, int, float const *, int, float*, cudnn::detail::impl...</td>\n",
       "      <td>108084</td>\n",
       "      <td>417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122541</td>\n",
       "      <td>1.109783e+07</td>\n",
       "      <td>2.368</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>void at::native::elementwise_kernel&lt;128, 4, at::native::gpu_kernel_impl&lt;at::native::gpu_kernel_with_scalars&lt;at::native::add_kernel_cuda(at::TensorIterator&amp;, c10::Scalar)::{lambda()#1}::operator()(...</td>\n",
       "      <td>108113</td>\n",
       "      <td>418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122562</td>\n",
       "      <td>1.109789e+07</td>\n",
       "      <td>35.583</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.140625</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>void cudnn::detail::bn_fw_tr_1C11_singleread&lt;float, int=512, bool=1, int=1, int=2, int=0&gt;(cudnnTensorStruct, float const *, cudnn::detail::bn_fw_tr_1C11_singleread&lt;float, int=512, bool=1, int=1, i...</td>\n",
       "      <td>108203</td>\n",
       "      <td>419</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               start  duration  registersPerThread  staticSMem  dynamicSMem  \\\n",
       "122257  1.109490e+07     2.400                16.0    0.000000          0.0   \n",
       "122278  1.109496e+07   204.607                40.0    0.140625          2.0   \n",
       "122350  1.109518e+07    25.504                16.0    0.000000          0.0   \n",
       "122372  1.109524e+07  2214.228                63.0    2.250000          0.0   \n",
       "122412  1.109747e+07     2.368                16.0    0.000000          0.0   \n",
       "122429  1.109753e+07    36.127                40.0    0.140625          2.0   \n",
       "122458  1.109761e+07   197.823                63.0    2.250000          0.0   \n",
       "122541  1.109783e+07     2.368                16.0    0.000000          0.0   \n",
       "122562  1.109789e+07    35.583                40.0    0.140625          2.0   \n",
       "\n",
       "        size  throughput  context  stream  \\\n",
       "122257   NaN         NaN      1.0     7.0   \n",
       "122278   NaN         NaN      1.0     7.0   \n",
       "122350   NaN         NaN      1.0     7.0   \n",
       "122372   NaN         NaN      1.0     7.0   \n",
       "122412   NaN         NaN      1.0     7.0   \n",
       "122429   NaN         NaN      1.0     7.0   \n",
       "122458   NaN         NaN      1.0     7.0   \n",
       "122541   NaN         NaN      1.0     7.0   \n",
       "122562   NaN         NaN      1.0     7.0   \n",
       "\n",
       "                                                                                                                                                                                                           name  \\\n",
       "122257  void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl<at::native::gpu_kernel_with_scalars<at::native::add_kernel_cuda(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()(...   \n",
       "122278  void cudnn::detail::bn_fw_tr_1C11_singleread<float, int=512, bool=1, int=1, int=2, int=0>(cudnnTensorStruct, float const *, cudnn::detail::bn_fw_tr_1C11_singleread<float, int=512, bool=1, int=1, i...   \n",
       "122350  void at::native::elementwise_kernel<512, 1, at::native::gpu_kernel_impl<at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}>(at::TensorIterator&, ...   \n",
       "122372  void cudnn::detail::implicit_convolve_sgemm<float, float, int=1024, int=5, int=5, int=3, int=3, int=3, int=1, bool=1, bool=0, bool=1>(int, int, int, float const *, int, float*, cudnn::detail::impl...   \n",
       "122412  void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl<at::native::gpu_kernel_with_scalars<at::native::add_kernel_cuda(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()(...   \n",
       "122429  void cudnn::detail::bn_fw_tr_1C11_singleread<float, int=512, bool=1, int=1, int=2, int=0>(cudnnTensorStruct, float const *, cudnn::detail::bn_fw_tr_1C11_singleread<float, int=512, bool=1, int=1, i...   \n",
       "122458  void cudnn::detail::implicit_convolve_sgemm<float, float, int=1024, int=5, int=5, int=3, int=3, int=3, int=1, bool=1, bool=0, bool=1>(int, int, int, float const *, int, float*, cudnn::detail::impl...   \n",
       "122541  void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl<at::native::gpu_kernel_with_scalars<at::native::add_kernel_cuda(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()(...   \n",
       "122562  void cudnn::detail::bn_fw_tr_1C11_singleread<float, int=512, bool=1, int=1, int=2, int=0>(cudnnTensorStruct, float const *, cudnn::detail::bn_fw_tr_1C11_singleread<float, int=512, bool=1, int=1, i...   \n",
       "\n",
       "        corrid  joinID  \n",
       "122257  107740     411  \n",
       "122278  107830     412  \n",
       "122350  107854     413  \n",
       "122372  107905     414  \n",
       "122412  107934     415  \n",
       "122429  108024     416  \n",
       "122458  108084     417  \n",
       "122541  108113     418  \n",
       "122562  108203     419  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(b2df[(b2df['joinID'] > 410) & (b2df['joinID'] <420)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
